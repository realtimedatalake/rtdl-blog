---
title: 'Announcing rtdl v0.1.0'
date: '2022-02-22'
tags: ['product', 'launch']
draft: false
summary: "rtdl v0.1.0 is now available. It's the easiest way to build and maintain a real-time data lake, and it works on AWS, GCP, and Azure."
images: ['/blog/static/images/blog/20220222-announcing-rtdl-v0-1-0/og-image.png',]
authors: ['johnson-gavin',]
---
![banner](/blog/static/images/blog/20220222-announcing-rtdl-v0-1-0/og-image.png)  
# Announcing rtdl v0.1.0
***The easiest way to build a real-time data lake on AWS, GCP, and Azure***  
  
About 6 months ago, I had a question that I couldn’t find a great answer to. I’d spent the last 
few years of my career working in tech for companies that made data tools. I’d become familiar 
with the tool ecosystem and common use cases across a data stack. Yet, I couldn’t figure out why 
getting real-time data – or at least near real-time data – for analysis was so difficult.  
  
Databricks is making data lakes and the lakehouse architecture popular, but even their real-time 
solutions take a lot of work to set up and frequently can’t get latencies down to seconds or a minute.  
  
I knew that solutions existed. I’d seen parts of them and had them detailed by data teams at many 
different companies. They are all custom-built though. Why aren’t there any tools available that 
you can just plug a Kafka stream or a webhook from Segment into that will automatically build you 
a real-time data lake?  
  
Now there is. [rtdl](https://rtdl.io) is the easiest way to build and maintain a real-time data 
lake. It works on AWS, GCP, and Azure, and it’s open source.

## rtdl v0.1.0 is now available
rtdl is the real-time data lake. You send rtdl a real-time data stream – often from a tool like 
Kafka or Segment – and it builds you a real-time data lake in Parquet format that automatically 
works with [Dremio](https://www.dremio.com/) to give you access to your real-time data in popular 
BI and ML tools – just like a data warehouse. rtdl can build your real-time data lake on AWS S3, 
GCP Cloud Storage, and Azure Blob Storage.

## How does rtdl work?
You configure a stream in rtdl. A stream includes authentication information for the data store 
your data lake will be built in, data lake configuration options, and identifiers – an id and an 
alternate id, like a Write Key or Source ID from Segment – to be included in data payloads sent 
to rtdl. Once a stream is configured, any JSON payload sent to rtdl that includes the same id 
or alternate id as the stream will be processed into Parquet and stored in the data store and 
as defined in the stream.  
  
As your data lake is being built, it is also configured in Dremio. So you can query your data in 
real-time via Dremio’s web UI or your favorite SQL, BI, and analytics tools with Dremio’s ODBC 
and JDBC drivers.

## What’s new in v0.1.0?
Three major enhancements that come with v0.1.0 and also round out rtdl’s initial feature set.
  1.  Added support for Azure Blob Storage V2. Previously, rtdl only supported AWS and GCP. Note 
      that it can take up to 1 minute for events written to Azure Blob Storage V2 to reflect in 
      Dremio, a latency that AWS S3 and GCP Cloud Storage don’t have.
  2.  Added support for GZIP and LZO compression types. Previously, rtdl only supported the default 
      compression type, SNAPPY.
  3.  Added support for Segment Webhooks destination as a source. Now you can point a Webhooks 
      destination in Segment at rtdl and it will build your real-time data lake with data from 
      Segment.

## Try rtdl today
Do you want real-time data or a data lake but don’t have the expertise or time to build your own 
solution? Or are you just tired of the toil required to build and maintain a data lake? You should 
try rtdl. It’s the easiest way to build a real-time data lake. rtdl works with all of the most 
popular cloud providers. Best of all, [it’s open source on GitHub](https://github.com/realtimedatalake/rtdl).  
  
Go try rtdl today.
